---
title: "PPforest"

 
---
A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest. The methods are implemented into an R package, called PPforest, which is available on CRAN. 

## Package functions
This is the second version of the package, it is a more simple and efficient version since we change the previous functions that use `plyr` to `dplyr`.

1. `var_select` random selection of variables to use in each node spliting
2. `train_fn` get a random sample by strata a training set with a sample proportion
3. `boostrap_pp` bootstrap samples with strata option
4. `trees_pp` for each bootstrap sample get a projection pursuit tree
5. `PPtree_split` Find tree structure using LDA or PDA index in each split. 
6. `forest_ppred` prediction for the PPforest
7. `PPforest` Summary results for a projection pursuit forest.
8. `PPclassify` 

## Function examples
I will do the first example with Fisher's Iris data.
```{r,echo=FALSE}
library(PPforest)
library(PPtreeViz)
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(gridExtra)
```


```{r,fig.show='hold',fig.width = 3 ,fig.height = 3 }
ggplot( data = iris,aes(x=Sepal.Length, y=Petal.Length, color = Species))+
  geom_point()+ theme(legend.position = "bottom")
ggplot( data = iris,aes(x=Sepal.Width, y=Petal.Width, color = Species))+
  geom_point()+ theme(legend.position = "bottom")
```


1. `var_select` random selection of variables to use in each node spliting
this function return a sample of id variables and has two arguments the data without class variable and the sample size proportion we want. We have to include a way to select the optimal sample size proportion base for example in cross validation.

```{r,eval=TRUE,collapse=TRUE}
head(iris[,-1])
x.iris <- iris[,-1]
set.seed(123)
variables <- PPforest::var_select(data=x.iris,size.p=0.7)
     variables
names(x.iris[,variables])

```
The selected variables are Petal.Length and  Petal.Width, we have to mention that we thake the floor of the sample proportion, this is the resason we got 2 out of 4 variables usen a size.p (sample proportion) of 0.7.
2. `train_fn` get a random sample by strata if we want a training set

```{r,collapse=TRUE}
set.seed(123)
 training <- PPforest::train_fn(class=iris[,5],size.p=0.7)
 with(iris[training$id, c(variables,5)],table(Species))
```
We can see that the sample is stratified by class.
```{r,fig.show='hold',fig.width = 7 ,fig.height = 7}
train <- iris %>% 
        dplyr::slice(as.numeric(training$id)) %>% 
        dplyr::mutate(sam="sample")

test <- iris %>%
      dplyr::slice(-as.numeric(training$id)) %>%
      dplyr::mutate(sam="test")

data.pl <- dplyr::rbind_list(train,test)
data.pl.melt <- reshape2::melt(data.frame(data.pl),id.vars=c("Species","sam"))


ggplot( data = data.pl.melt,aes(x=Species, y=value,color = sam))+ ylab("")+
  geom_dotplot(binaxis="y", stackdir="center", binwidth=0.15) +
         labs(title="Dot plots")+ theme(legend.position = "bottom") +facet_wrap(~variable)

```
3. `boostrap_pp` bootstrap samples with strata option
```{r,fig.show='hold',fig.width = 3 ,fig.height = 3}
iris.b <- PPforest::bootstrap(iris[,5:1], 5) 
index <- base::lapply(attributes(iris.b)$indices ,function(x) x+1)
boot.dat <- base::lapply(index,function(x) iris[x,])
aux1 <- base::lapply(boot.dat,function(x) melt(x,id.vars=c("Petal.Width","Petal.Length","Species")))

```
```{r, fig.show='hold',fig.width = 7 ,fig.height = 4,echo=FALSE}
p1 <- ggplot( data = iris,aes(x=Petal.Width, y=Petal.Length, color = Species))+
  geom_point()+ theme(legend.position = "bottom") +ggtitle("Iris Data")
p1
```
```{r, fig.show='hold',fig.width = 7 ,fig.height = 7,echo=FALSE}
p2 <- ggplot( data = aux1[[1]],aes(x=Petal.Width, y=Petal.Length, color = Species))+geom_point()+ theme(legend.position = "bottom")+ggtitle("Replicate 1")
p3<- ggplot( data = aux1[[2]],aes(x=Petal.Width, y=Petal.Length, color = Species))+geom_point()+ theme(legend.position = "bottom")+ggtitle("Replicate 2")
p4<- ggplot( data = aux1[[3]],aes(x=Petal.Width, y=Petal.Length, color = Species))+geom_point()+ theme(legend.position = "bottom")+ggtitle("Replicate 3")
p5<- ggplot( data = aux1[[4]],aes(x=Petal.Width, y=Petal.Length, color = Species))+geom_point()+ theme(legend.position = "bottom")+ggtitle("Replicate 4")

grid.arrange(p2, p3,p4,p5,heights=c(.7, .7,.7,.7))

```
4.`PPtree_split` projection pursuit tree with random variable spliting in each node
```{r}
 training <- PPforest::train_fn(iris[,5],.9)
 data1<-iris[,5:1]
Tree.result <- PPforest::PPtree_split(as.formula('Species~.'), data=data1 , std = TRUE,  size.p=0.9)
 Tree.result
```

5. `trees_pp` for each bootstrap sample get a projection pursuit tree
```{r}
output <- PPforest::trees_pp(iris.b,size.p=.9,index="LDA") 
names(output)
```
The PPtrees object are in output$tr
6. `forest_ppred` prediction for the PPforest

```{r,collapse=TRUE}
iris.sc <- data.frame(Class = iris[,5], scale(iris[,1:4]))
training <- PPforest::train_fn(class=iris[,5] , size.p=0.9)
iris.b <- PPforest::bootstrap( iris.sc[training$id,], 10) 
output <- PPforest::trees_pp(iris.b,size.p=0.9,index="LDA") 
pred <- PPforest::forest_ppred( iris.sc[-training$id,2:5] , output)

t1 <- table(pred[[3]],iris.sc[-training$id,1])
t1

error <-1-sum(diag(t1))/length(iris.sc[-training$id,1])
error
```


7. `PPforest` prediction for the PPforest
```{r,collapse=TRUE,cache=TRUE}
 tr.index <- PPforest::train_fn(iris[, 5], 0.9)
 te.index <- as.vector(1:length(iris[, 5]))[!(1:length(iris[, 5]) %in% (sort(tr.index$id)))]
 train <- iris[sort(tr.index$id), 5:1 ]
test <- iris[-tr.index$id, 5:1 ]

 ppfr.iris <- PPforest::PPforest( train = train, testap = TRUE, test = test, m = 500, size.p = .9, PPmethod = 'LDA', strata = TRUE)

```


```{r, collapse=TRUE, cache=TRUE, fig.width = 5 ,fig.height = 3}
 grilla <- base::expand.grid(x3=seq(0,10,,100), x4=seq(0,10,,100), x1=3, x2=5.8)
 colnames(grilla)<-colnames(train)[-1]
 pred <- forest_ppred( grilla , ppfr.iris[[8]] )
 grilla$pred<-pred[[3]]
 
 p<- ggplot(data = grilla ) + geom_point(aes(x =  Petal.Length, y = Petal.Width, color = as.factor(pred)),alpha = .5) + scale_shape_discrete(name = "Type") + scale_colour_discrete(name  ="Type.bound") 
 p + geom_point(data = train, aes(x = Petal.Length , y = Petal.Width, shape = as.factor(Species)), size = I(2)  ) +  theme(legend.position = "none")

 
 ```
 
 
```{r, collapse=TRUE, cache=TRUE, fig.width = 5 ,fig.height = 3}
 grilla <- base::expand.grid(x3=1.3, x4=4.4, x1=seq(0,10,,100), x2=seq(0,10,,100))
 colnames(grilla)<-colnames(train)[-1]
 pred <- PPforest::forest_ppred( grilla , ppfr.iris[[8]] )
 grilla$pred<-pred[[3]]
 
 p<- ggplot(data = grilla ) + geom_point(aes(x =  Sepal.Width, y = Sepal.Length  , color = as.factor(pred)),alpha = .5) + scale_shape_discrete(name = "Type") + scale_colour_discrete(name  ="Type.bound") 
 p + geom_point(data = train, aes(x =Sepal.Width , y = Sepal.Length, shape = as.factor(Species)), size = I(2)  ) +  theme(legend.position = "none")

 
 ```