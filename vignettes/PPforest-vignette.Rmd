---
title: "Projection pursuit classification random forest "
author: "N. da Silva; E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::docco_linear}}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
## Abstract
A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest. The methods are implemented into an R package, called PPforest, which is available on CRAN. 

_Keywords_: Random forest, projection pursuit, supervised classification, exploratory data analysis, data mining, visualization


## Introduction

A random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest) will be describe. For each split a random sample of variables is selected and a linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest.

Projection pursuit random forest algorithm description

1. Let N the number of cases in the training set, bootstrap samples from the training set are taking (samples of size N with replacement)

2. For each bootstrap sample a PPtree type algorithm is grown to the largest extent possible. The original PPtree algorithm select in each node the optimal one-dimension projection for separating all classes in the data. PPforest need a modification of PPtree algorithm,  we select for each split a random sample instead of using all the variables and after that in each node we select the optimal one-dimension projection for separating all classes.

3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and each split is based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.

We present two examples using real data to show how PPforest package works.

##Usage
The following table has the PPforest functions with a description.

| Function |Description |
| ------ | ------ | -----: |
|  var_select  |Index id for variables set, sample variables without replacement with constant sample proportion |
|  train_fn |Index id for training set, sample in each class with constant sample proportion. |
|PPtree_split|Projection pursuit classification tree with random variable selection in each split|
|trees_pp| Projection pursuit trees for bootstrap samples|
|importance|Data frame with the 1D projection of each split unweighted and weighted by 1-(oob error rate) for each tree and plots of importance variable for each split.|
|PPforest|Run a Projection Pursuit forest|
|forest_ppred|Vector with predicted values from a PPforest|

##Examples
Data sets:

1. NCI 60: Gene expression data set, contains 61 observations and 30 feature variables. Class variable has 8 different tissue types, 9 breast, 5central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non- small-cell lung carcinoma (NSCLC), 6 ovarian and 9 renal. There are 6830 genes.
 
2. Crab: contains 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BM, BF, OM and OF). There are 5 continuous feature variables. 
FL is the size of the frontal lobe, RW is rear width, CL is carapace length, CW width and BD body depth.

The PPforest function runs a projection pursuit random forest, the argument testap=TRUE allows to use test data. In these two examples we split the data in training (2/3) and test (1/3),  using train_fn function we get the index id for training set, this function sample in each class with the same proportion.

###NCI60 Data
```{r}

training.id <- PPforest::train_fn(NCI60[, 1], 2/3)
 test.id <- as.vector(1:length(NCI60[, 1]))[!(1:length(NCI60[, 1]) %in% (sort(training.id$id)))]
 training.nci <- NCI60[sort(training.id$id), ]
 test.nci <- NCI60[-training.id$id, ]
 pprf.nci60 <- PPforest::PPforest(train = training.nci, testap = TRUE, test = test.nci, m = 500, size.p = .2, PPmethod = 'PDA', strata = TRUE, lambda=.7)

```

PPforest function returns the predicted values of the training data, training error,  if test data was provided test error and predicted test values are returned, the forest out of bag error, the out of bag error for each tree, the bootstrap samples, the output of all the trees in the forest from trees_pp function, proximity matrix and vote matrix.

```{r}
str(pprf.nci60,max.level=1)
```

From PPforest object we can plot a heat map of the proximity matrix using the function proxy_plot.

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::pproxy_plot( pprf.nci60 , training.nci)
```

We can also plot the MDS of the proximity matrix using the function PPplot.
If we select k=2 the output plot is as follows:

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::PPplot(pprf.nci60, training.nci, k = 2)
```
If we select k>2 the graphical display will be:

```{r,fig.show='hold',fig.width = 6 ,fig.height = 6,echo=FALSE}
PPforest::PPplot(pprf.nci60, training.nci, k = 3)
```

###Crab Data
```{r}
 training.id <- PPforest::train_fn(crab[, 1], 2/3)
 test.id <- as.vector(1:length(crab[, 1]))[!(1:length(crab[, 1]) %in% (sort(training.id$id)))]
 training.crab <- crab[sort(training.id$id), ]
 test.crab <- crab[-training.id$id, ]
 pprf.crab <- PPforest::PPforest(train = training.crab, testap = TRUE, test = test.crab, m = 500, size.p = .7, PPmethod = 'LDA', strata = TRUE)
str(pprf.crab,max.level=1)
```

```{r,fig.show='hold',fig.width = 4 ,fig.height = 3,echo=FALSE}
PPforest::pproxy_plot( pprf.crab , training.crab)
```

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::PPplot(pprf.crab, training.crab, k = 2)
```
If we select k>2 
```{r,fig.show='hold',fig.width = 7 ,fig.height = 7,echo=FALSE}
PPforest::PPplot(pprf.crab, training.crab, k = 3)
```


