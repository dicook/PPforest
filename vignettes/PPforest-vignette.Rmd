---
title: "Projection pursuit classification random forest "
author: "N. da Silva, E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette Title}
  \usepackage[utf8]{inputenc}
---

## Abstract

A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (`PPforest`). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the `PPforest`, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The `PPforest` uses the `PPtree` algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for `PPforest`. 

##Introduction
Random Forest was proposed by Leo Brieman (2001), there are two main concept used in random forest, bootstrap aggregation  and random feature selection  to individual classification or regression trees for prediction.  The most popular random forest uses univariate decision trees (CART or C4.5), these trees select a single feature variable in each split. A not very popular version of random forest that is also presented in the original paper, uses  oblique trees where the feature space is selected by random selected hyperplanes.

The approach presented here (`PPforest`) goes in the spirit of the second random forest version. Our porpoise works well when the data are collinear with correlated features. In these cases hyperplanes that are oblique to the axis maybe do a better work in separate the feature space.

###Motivation Example
Australian crab data set contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BlueMale, BlueFemale, OrangeMale and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.
 
1. FL the size of the frontal lobe length, in mm
2. RW rear width, in mm
3. CL length of mid line of the carapace, in mm
4. CW maximum width of carapace, in mm
5. BD depth of the body; for females, measured after displacement of the abdomen, in mm

To visualize this data set we use a scatterplot matrix.

```{r,fig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE}
load("../data/crab.rda")
#library(RColorBrewer)
#library(GGally)

# ggpairs(crab, 
#     columns= 2:6,
#     ggplot2::aes(colour=Type),
#     lower=list(continuous='points'), 
#     axisLabels='none',  
#     upper=list(continuous='blank') 
#      ) 
```

```{r, fig.show='hold',fig.width = 7 ,fig.height = 7, echo=FALSE, message=FALSE}
# library(ggplot2)
# library(RColorBrewer)
# crab2 <- base::subset(crab, select=c(Type,RW,BD))
# grilla <- base::expand.grid(RW=seq(6,21,,50), BD=seq(6,22,,50))
# 
# pptree <- PPtreeViz::PP.Tree.class(crab2[,1],crab2[,-1],"LDA")
# 
# ppred.crab <- PPtreeViz::PP.classify( pptree, test.data=grilla)
# grilla$ppred <- ppred.crab[[2]]
# 
# rpart.crab <- rpart::rpart(Type ~ RW + BD, data=crab)
# rpart.pred <- predict(rpart.crab, newdata = grilla, type="class")
```

```{r, fig.show='hold',fig.width = 7, fig.height = 4, echo=FALSE, message=FALSE,warning=FALSE,eval=FALSE}

#To better understand the main characteristics of `PPtree` algorithm we will use a subset of crab data. We have selected two feature variables, RW and BD. With this reduced data set we run a `rpart` and a `PPtree` and we plot the decision boundaries for each tree.


# p <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x =RW, y = BD  , color = as.factor(ppred), ratio = 1),alpha = .50)+ ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw() 
#   
# 
# 
# pl.pp <- p + ggplot2::geom_point(data = crab2, ggplot2::aes(x =RW , y = BD, group=Type,shape = Type, color=Type), size = I(3)  ) +  ggplot2::scale_shape_manual(name = "Class", labels = levels(crab2$Type),values = c(19, 18, 15, 17)) + ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw()+  ggplot2::scale_colour_manual(name = "Class",
#                       labels = levels(crab2$Type),
#                       values = c("#1B9E77" ,"#D95F02" ,"#7570B3" ,"#E7298A")) +
#   ggplot2::theme(legend.position = "bottom")
# 
#  p2 <- ggplot2::ggplot(data = grilla ) + ggplot2::geom_point(ggplot2::aes(x =RW, y = BD  , color = as.factor(rpart.pred), ratio = 1), alpha = .5) + ggplot2::scale_colour_brewer(type="qual",palette="Dark2") + ggplot2::theme_bw() 
#   
# 
#  pl.rpart <- p2 + ggplot2::geom_point(data = crab2, ggplot2::aes(x =RW , y = BD, group=Type,shape = Type, color=Type), size = I(3)  ) +  ggplot2::scale_shape_manual(name = "Class", labels = levels(crab2$Type),values = c(19, 18, 15, 17)) + ggplot2::scale_colour_manual(name = "Class",
#                       labels = levels(crab2$Type),
#                       values =  c("#1B9E77" ,"#D95F02" ,"#7570B3" ,"#E7298A")) + ggplot2::theme(legend.position = "bottom")
# 
# 
# 
# gridExtra::grid.arrange(pl.rpart,pl.pp, nrow=1)

#The  left panel shows the `rpart` decision boundaries while the right panel shows the `PPtree` decision boundaries.
#Decision boundaries from  `rpart` are orthogonal to the axis and for this data set several splits are needed in order to capture the data structure. The structure of the decision boundaries generated by `rpart` doesn't corresponds with the data. On the other hand, `PPtree` the decision boundaries defined by `PPtree` follows the structure defined by the data. We can see that `PPtree` require less splits to capture the data structure.

```

##Projection pursuit classification forest
 `PPforest` implements a projection pursuit classification random forest. Projection pursuit classification trees  are used to build the forest, ( from `PPtreeViz` package ). `PPforest` adapts random forest to utilize combinations of variables in the tree construction.

`PPforest` is generated form `PPtree` algorithm. `PPtree` combines tree structure methods with projection pursuit dimensional reduction.


One important characteristic of PPtree is that treats the data always as a two-class system,  when the classes are more than two the algorithm uses a two step  projection pursuits optimization in every node split. 
Let  $(X_i,y_i)$ the data set, $X_i$ is a  p-dimensional vector of explanatory variables and  $y_i\in {1,2,\ldots G}$ represents class information with $i=1,\ldots n$.

In the first step optimize a projection pursuit index to find an optimal one-dimension projection $\alpha^*$ for separating all classes in the current data. With the projected data redefine the problem in a two class problem by comparing means, and assign a new label $G1$ or $G2$ to each observation, a new variable $y_i^*$ is created.  The new groups $G1$ and $G2$ can contain more than one original classes. Next step is to find an optimal one-dimensional projection $\alpha$, using $(X_i,y_i^*)$ to separate the two class problem $G1$ and $G2$. The best separation of $G1$ and $G2$ is determine in this step and the decision rule is defined for the current node, if $\sum_{i=1}^p \alpha_i M1< c$ then assign $G1$ to the left node else assign $G2$ to the right node, where $M1$ is the mean of $G1$.
For each groups we can repeat all the previous steps until $G1$ and $G2$ have only one class from the original classes. Base on this process to grow the tree, the depth of PPtree is at most the number of classes.  


Trees from `PPtree` algorithm are simple, they use the association between variables to find separation. If a linear boundary exists, `PPtree` produces a tree without misclassification.
One class is assigned only to one final node, depth of the tree is at most the number of classes.
Finally the projection coefficient of the node represent the variable importance.

The following plots shows the tree structure from `rpart` and `PPtree`. Trees from `PPtree` are smaller and simpler than trees from `rpart`. For this example `PPtree` present three splits while `rpart` tree did  13 splits. 

```{r,  fig.show='hold',fig.width = 7, fig.height = 4, echo=FALSE, message=FALSE}
# library(rpart.plot)
# library(gridExtra)
# library(ggplot2)
# 
# pptree <- PPtreeViz::PP.Tree.class(crab[,1],crab[,-1],"LDA")
# rp <- rpart::rpart(Type~ .,data=crab)
# 
# pl2 <- rpart.plot::rpart.plot(rp)
# pl1 <- plot(pptree)


```


The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occur on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest.




Projection pursuit random forest algorithm description



1. Let N the number of cases in the training set $\Theta=(X,Y)$, $B$ bootstrap samples from the training set are taking (samples of size N with replacement)
2. For each bootstrap sample a \verb PPtree  is grown to the largest extent possible $h(x, {\Theta_k})$. No pruning. This tree is grown using step 3 modification.
3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and the best split based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.
4.  Predict the classes of each case not included in the bootstrap sample and compute oob error.
5.  Based on majority vote predict the class for new data.

###Overview PPforest package
`PPforest` package implements a classification random forest using projection pursuit classification trees. The following table present all the functions in `PPforest` package. 

| Function |Description |
| ------ | ------ | -----: |
|PPforest|Runs a Projection pursuit random forest|
|predict.PPforest|Vector with predicted values from a PPforest object|
|ppf_importance| Plot a global measure of importance in PPforest.|
|pproxy_plot| Proximity matrix visualization|
|ppf_oob_error| OOB error visualization|
|var_select  |Index id for variables set, sample variables without replacement with constant sample proportion |
|train_fn |Index id for training set, sample in each class with constant sample proportion. |
|PPtree_split|Projection pursuit classification tree with random variable selection in each split|
|trees_pp| Projection pursuit trees for bootstrap samples|
|tree_ppred|Vector with predicted values from a PPtree object.|
|ppf_bootstrap| Draws bootstrap samples with strata option.|
|print.PPforest| Print PPforest object|

Also `PPforest` package includes some data set that were used to test the predictive performance of our method. The data sets included are: crab, fishcatch, glass, image, leukemia, lymphoma NCI60, parkinson and wine.

We continue with crab data example to show how `PPforest` works, we first split the data in training (2/3) and test (1/3).
To do that we use `train_fn` function, this function has three arguments, data is a data frame with the complete data set, class is a character withthe name of the class variable in my data set and size.p is the sample proportion we want. The sample is stratified by each class with the same proportion. This function returns a vector with the sample ids.

```{r}
# training.id <- PPforest::train_fn(data = crab, class= "Type", size.p = 2/3)

```
If we want all the selected id's we use `training.id$id`

```PPforest``` function runs a projection pursuit random forest. This function also has a data argument and class argument. Using this function we have the option to split the data in training and test using size.tr directly. `size.tr` is the size proportion of the training then the proportion of this will be 1- `size.tr`.
The number of trees in the forest is specified using the argument m. The argument size.p is the sample proportion of the variables used in each node split, `PPmethod` is the projection pursuit index to be optimized there are two options LDA and PDA. Finally strata=TRUE indicate that the bootstrap samples are stratified by class variable. 

```{r}
# set.seed(146)
# pprf.crab <- PPforest::PPforest(data = crab, class = "Type", size.tr = 2/3, m = 500, size.p =  .7,  PPmethod = 'LDA', strata = TRUE)
# 

```

`PPforest` function returns the predicted values of the training data, training error, test error and predicted test values are returned. Also there is the information about out of bag error for the forest and also for each tree in the forest. Bootstrap samples, output of all the trees in the forest from trees_pp function, proximity matrix and vote matrix, number of trees grown in the forest, number of predictor variables selected to use for splitting at each node. Confusion matrix of the prediction (based on OOb data), the training data and test data and vote matrix are also returned.

The printed version of a `PPforest` object follows the `randomForest` printed version to make them comparable. Based on confusion matrix, we can observe that the biggest error is for BlueMale class. Most of the wrong classified values are between BlueFemale and BlueMale.
```{r}
# pprf.crab
```



If we compare the results with the `randomForest` function for this data set the results are the following:
```{r}
# rf.crab <- randomForest::randomForest(Type~., data = crab, proximity = TRUE)
# rf.crab
```
We can see that for this data set the `PPforest` performance is much better than using `randomForest`. `PPforest` works well since the classes can be separated by linear combinations of variables. 
This is a clear case where oblique hyperplanes are more adequate in this case than hyperplanes horizontal to the axis.

We can get the predicted values for training and test using the output of `PPforest`

```{r}
#  pred.training <- pprf.crab$prediction.training
#  pred.test <- pprf.crab$prediction.test
#  pred.test[1:10]
```

From `PPforest` object we can plot a heat map of the proximity matrix using the function `pproxy_plot`.

```{r,fig.width = 4 ,fig.height = 3,echo=FALSE}
#PPforest::pproxy_plot(pprf.crab, type="heat", inter=FALSE)
```

The data are ordered by class (BlueFemale, BlueMale, OrangeFemale and OrangeMale), in the heat map we can observe a colored block diagonal structure, this means that observations form the same class are similar, but also we can observe that observations from BlueMale and BlueFemale are similar (big blue square in the left top corner).

Additionally `pproxy_plot` can be used to plot the MDS using proximity matrix information.

If we select k=2 the output plot is as follows:

```{r,fig.show='hold',fig.width = 7 ,fig.height = 4}
#PPforest::pproxy_plot(pprf.crab, type="MDS", k=2,inter=FALSE)
```

We can observe a spatial separation between classes. Orange (male and female) are more separated than Blue (male and female).

If we select k>2,  we can observe that using two dimensions is enough to see the spatial separation.

```{r,fig.show='hold',fig.width = 6 ,fig.height = 6}
#PPforest::pproxy_plot(pprf.crab, type="MDS",k = 3)
```

A global measure of importance is computed. In `PPtree` the projection coefficient of each node represent the importance of variables to class separation in each node. To get a global measure in `PPforest` we need to take into account the importance in each node and combine the results for all the trees in the forest. The importance measure of `PPforest` is a  weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are  the projection pursuit index in each node, and 1-the out of bag error of each tree.

Using the `ppf_importance` function we can plot the global variable importance measure in `PPforest`.

```{r, fig.show='hold',fig.width = 4 ,fig.height = 3}
# PPforest::ppf_importance(y = crab[, 1], x = crab[, -1], pprf.crab, global = TRUE, weight = TRUE, inter=FALSE) 
```

We can see that the most important variable in this example is RW while the less important is BD

Finally a cumulative out of error plot can be done using `ppf_oob_error`

```{r, fig.show='hold',fig.width = 6 ,fig.height = 4}
# PPforest::ppf_oob_error(pprf.crab, nsplit1 = 10, nsplit2 = 100)
```

The oob-error rate decrease when we increase the numbers of trees but gets constant with less than 250 trees. 
 
##NCI60 example

Gene expression data set, contains 61 observations and 30 feature variables. Class variable has 8 different tissue types, 9 breast, 5central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non- small-cell lung carcinoma (NSCLC), 6 ovarian and 9 renal. There are 6830 genes.

Since this data set present a considerable big number of predictive variables we will explore the data set using `PPTreeViz` package first.

We can explore the optimal projection for classification in q-dimensional space. We select q=1 and PDA projection pursuit index. Whit q=1 `PPopt.Viz` returns an histogram with the projected data and the plot  with the coefficients of projection. 


```{r,  fig.show='hold',fig.width = 7 ,fig.height = 4, message=FALSE}
# library(ggplot2)
# load("../data/NCI60.rda")
# PPtreeViz::PPopt.Viz(PPtreeViz::PDAopt(NCI60[, 1], as.matrix(NCI60[, -1]), q = 1))

```

In the histogram we can observe each class distribution and we can observe that leukemia is separate from the other classes. In the plot with the coefficient values we can observe that Gene 30 and Gene 28 are the most important to separate the leukemia class in this view.  

We can use a scatterplot matrix with some of these Gene variables that are important for the separation. We can see in this plot that the data are correlated and the green class is different from the rest (leukemia).
```{r, fig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE}
# ggpairs(NCI60, 
#     columns= 27:31,
#     colour='Type',
#     lower=list(continuous='points'), 
#     axisLabels='none',
#     upper=list(continuous='blank')
# )
```

```{r, fig.show='hold', fig.width = 10, fig.height = 5, echo=FALSE}
# gpd <-ggparcoord(data = NCI60, columns = 2:31, groupColumn = 1, order = "anyClass", showPoints = TRUE) + theme(axis.text.x = element_text(angle = 90), legend.position="bottom")
#  gpd
```

If we select q=2, `PPopt.Viz shows the best 2 dimensional projection coefficients and the scatter plot of the projected data. We can observe that colon and leukemia are separable.
```{r, fig.show='hold',fig.width = 7 ,fig.height = 4, message=FALSE}
# library(ggplot2)
# PPtreeViz::PPopt.Viz(PPtreeViz::PDAopt(NCI60[, 1], as.matrix(NCI60[, -1]), q = 2))
```

In the same way than for crab data we split the data in training and test(size.tr=2/3) and we run a 
`PPforest`. In this case PDA index are more appropriate since the number of observations small related to the number of feature variables. We select $\lambda=.7$, it is a penalty parameter in PDA index and is between 0 to 1. size.p=.7 is the random sample variable proportion used in each node split.

```{r}
# set.seed(123)
# NCI60 <- dplyr::arrange(NCI60, desc(Type))
# pprf.nci60 <- PPforest::PPforest(data = NCI60, class = "Type", size.tr = 2/3, m = 500, size.p = .7, PPmethod = 'PDA', strata = TRUE, lambda=.5)
# pprf.nci60
```

We can compare the `PPfores` performance with `randomForest`
```{r}
# set.seed(123)
# rf.nci60 <- randomForest::randomForest(Type~., data = NCI60, proximity = TRUE)
# rf.nci60
```

In this case `PPforest` also shows a better performance than `randomForest`.

We can also plot the MDS of the proximity matrix using the function `pproxy_plot`.
If we select k=2 we can see a block diagonal structure with 8 blocks that are related with the order of the classes (Renal, Ovarian, NSCLC, Melanoma, Leukemia, Colon CNS and Breast). Colon and leukemia are classes that are different to the rest of the classes and this result is consistent with the previous 2 dimension projection plot were we saw that leukemia and colon are separable.  
```{r,v21,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
#PPforest::pproxy_plot( pprf.nci60, type="heat" )
```



```{r, fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
#PPforest::pproxy_plot(pprf.nci60, type="MDS",k = 2)
```

The importance variable plot shows that the most important variables are Gene26 to Gene30 while Gene9 and Gene16 are the less important for the classification `PPforest`.
```{r,fig.show='hold',fig.width = 6 ,fig.height = 6,echo=FALSE}
# PPforest::ppf_importance(y = NCI60[ ,1], x = NCI60[, -1], pprf.nci60, global = TRUE, weight = TRUE) 
```

The cumulative oob error plot shows that ovarian class has a constant error of 1 while for other classes like renal, melanoma and NSCLC show a decrease error when the number of trees increase.
  

```{r,v24, fig.show='hold',fig.width = 6 ,fig.height = 4}
#PPforest::ppf_oob_error(pprf.nci60, nsplit1 = 10, nsplit2 = 100)   
```

