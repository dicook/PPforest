---
title: "Projection pursuit classification random forest "
author: "N. da Silva, E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::docco_linear}
  %\VignetteEncoding{UTF-8}
---
## Introduction

A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest. 

Projection pursuit random forest algorithm description

1. Let N the number of cases in the training set, bootstrap samples from the training set are taking (samples of size N with replacement)

2. For each bootstrap sample a PPtree type algorithm is grown to the largest extent possible. The original PPtree algorithm select in each node the optimal one-dimension projection for separating all classes in the data. PPforest need a modification of PPtree algorithm,  we select for each split a random sample instead of using all the variables and after that in each node we select the optimal one-dimension projection for separating all classes.

3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and each split is based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.

We present two examples using real data to show how PPforest package works.

##Overview PPforest package
PPforest package implements a classification random forest using projection pursuit classification trees. The following table present all the functions in PPforest package. 

| Function |Description |
| ------ | ------ | -----: |
|var_select  |Index id for variables set, sample variables without replacement with constant sample proportion |
|  train_fn |Index id for training set, sample in each class with constant sample proportion. |
|PPtree_split|Projection pursuit classification tree with random variable selection in each split|
|trees_pp| Projection pursuit trees for bootstrap samples|
|importance| Plot a global measure of imortance in PPforest.|
|PPforest|Run a Projection pursuit random forest|
|forest_ppred|Vector with predicted values from a PPforest object.|
|bootstrap| Draws bootstrap samples with strata option.|
|PPplot|Proximity matrix visualization using MDS|
|print.PPforest| Print PPforest object|

## Australian Crab example
We will explore PPforest package using Australian crab data set.
This data set contains measurements on rock crabs of the genus Leptograpsus. There are 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BlueMale, BlueFemale, OrangeMale and OrangeFemale). The data were collected on site at Fremantle, Western Australia. For each specimen, five measurements were made, using vernier calipers.
 
1. FL the size of the frontal lobe length, in mm
2. RW rear width, in mm
3. CL length of midline of the carapace, in mm
4. CW maximum width of carapace, in mm
5. BD depth of the body; for females, measured after displacement of the abdomen, in mm

To visualize this data set we use a scatterplot matrix.

```{r, ig.show='hold', fig.width = 6, fig.height = 6, echo=FALSE}
load("../data/crab.rda")
GGally::ggpairs(crab, 
    columns= 2:6,
    colour='Type',
    lower=list(continuous='points'), 
    axisLabels='none',  
    upper=list(continuous='blank')
)
```

To run the PPforest we first split the data in training (2/3) and test (1/3).
To do that we use train_fn function, this function has two arguments, class is a vector with the class variable and size.p is the sample proportion we want. The sample is stratified by each class with the same proportion. This function returns a vector with the sample ids.


```{r}
training.id <- PPforest::train_fn(crab[, 1], 2/3)
str(training.id)
training.id
```
If we want all the selected id's we use training.id$id

PPforest function runs a projection pursuit random forest,  using this function we have the option to split the data in training and test using size.tr and testap=TRUE directly here. size.tr is the size proportion of the training. If we want to use test data we have to specify  testap=TRUE and the proportion of this will be 1- sixe.tr.
The number of trees in the forest is specified using the argument m. size.p is the sample proportion of the variables used in each node split, PPmethod is the projection pursuit index to be optimized there are two options LDA and PDA. Finally strata=TRUE indicate that the bootstrap samples are stratified by class variable. 

```{r}
pprf.crab <- PPforest::PPforest(data = crab, size.tr = 2/3,
              testap = TRUE, m = 500, size.p = .7,  
              PPmethod = 'LDA', strata = TRUE)
str(pprf.crab, max.level = 1)
```
PPforest function returns the predicted values of the training data, training error,  if testap=TRUE then test error and predicted test values are returned. Also there is the information about out of bag error for the forest and also for each tree in the forest. Bootstrap samples, output of all the trees in the forest from trees_pp function, proximity matrix and vote matrix,  number of trees grown in the forest, number of predictor variables selected to use for splitting at each node. Confusion matrix of the prediction (based on OOb data), the training data and test data are also returned.

The printed version of a PPforest object follows the randomForest printed version to make them comparable
```{r}
pprf.crab
```

If we compare the results with the randomForest function for this data set the results are the following:
```{r}
rf.crab <- randomForest::randomForest(Type~.,data=crab,proximity=TRUE)
rf.crab
```
We can see that for this data set the \code{PPforest} performance is much better than using \code{randomForest}. \code{PPforest} works well since the classes can be separated by linear combinations of variables. 

We can get the predicted values for training and test using the output of PPforest 
```{r}
 pred.training <- pprf.crab$prediction.training
 pred.test <- pprf.crab$prediction.test
 pred.test[1:10]
```

From PPforest object we can plot a heat map of the proximity matrix using the function \code{proxy_plot}.

```{r,fig.show='hold',fig.width = 4 ,fig.height = 3,echo=FALSE}
PPforest::pproxy_plot( pprf.crab)
```

We can also plot the MDS using proximity matrix information with the function PPplot.

If we select k=2 the output plot is as follows:

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::PPplot(pprf.crab, k = 2)
```

If we select k>2 

```{r,fig.show='hold',fig.width = 7 ,fig.height = 7,echo = FALSE}
PPforest::PPplot(pprf.crab, k = 3)
```

A global measure of importance is computed. In PPtree the projection coefficient of each node represent the importance of variables to class separation in each node. To get a global measure in PPforest we need to take into account the importance in each node and combine the results for all the trees in the forest. The importance measure of PPforest is a  weighted mean of the absolute value of the projection coefficients across all nodes in every tree. The weights are  the projection pursuit index in each nide, and 1-the out of bag error of each tree.

Using the \code{importance} function we can plot the global variable importance measure in PPforest.

```{r}
PPforest::importance(crab, pprf.crab, global = TRUE, weight = TRUE) 
```

We can see that the most important variable in this example is RW while the less important is CL



##NCI60 example

Gene expression data set, contains 61 observations and 30 feature variables. Class variable has 8 different tissue types, 9 breast, 5central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non- small-cell lung carcinoma (NSCLC), 6 ovarian and 9 renal. There are 6830 genes.

We will split the data in training (2/3) and test(1/3).

```{r}
load("../data/NCI60.rda")
training.id <- PPforest::train_fn(NCI60[, 1], 2/3)
 training.nci <- NCI60[sort(training.id$id), ]
 test.nci <- NCI60[-training.id$id, ]
```
Since this data set present a considerable big number of predictive variables we will explore the data set using PPTreeViz package.

We can explore the optimal projection for classification in q-dimensional space. 


```{r}
#PPtreeViz::PPopt.Viz(PPtreeViz::LDAopt(NCI60[, 1], NCI60[, -1], q = 1))
#PPtreeViz::PPopt.Viz(PPtreeViz::PDAopt(NCI60[, 1], as.matrix(NCI60[, -1]), q = 1))

```

```{r}
set.seed(143)
pprf.nci60 <- PPforest::PPforest(data = NCI60, size.tr = 2/3, testap = TRUE,  m = 500, size.p = .2, PPmethod = 'PDA', strata = TRUE, lambda=.3)

```


```{r}
str(pprf.nci60,max.level=1)
pprf.nci60
```


We can compare the results with randomForest
```{r}
rf.nci60 <- randomForest::randomForest(Type~., data = NCI60, proximity = TRUE)
rf.nci60
```

In this case PPforest also shows a better performance than randomForest.

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::pproxy_plot( pprf.nci60 )
```

We can also plot the MDS of the proximity matrix using the function PPplot.

If we select k=2 the output plot is as follows:

```{r,fig.show='hold',fig.width = 5 ,fig.height = 4,echo=FALSE}
PPforest::PPplot(pprf.nci60, k = 2)
```
If we select k>2 the graphical display will be:

```{r,fig.show='hold',fig.width = 6 ,fig.height = 6,echo=FALSE}
PPforest::PPplot(pprf.nci60, k = 3)
```
And finally the global importance measure plot:


```{r,fig.show='hold',fig.width = 6 ,fig.height = 6,echo=FALSE}
PPforest::importance(NCI60, pprf.nci60, global = TRUE, weight = TRUE) 
```

We can observe that the most important variables are Gene28, Gene14 and Gene25 while Gene9 and Gene15 are the less important for the classification PPforest.
