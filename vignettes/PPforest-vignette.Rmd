---
title: "Projection pursuit classification random forest "
author: "N. da Silva; E. Lee & D. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::docco_linear}}
  %\VignetteEncoding{UTF-8}
---


## Abstract
A random forest is an ensemble learning method, built on bagged trees. The bagging provides power for classification because it yields information about variable importance, predictive error and proximity of observations. This research adapts the random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest). In a random forest each split is based on a single variable, chosen from a subset of predictors. In the PPforest, each split is based on a linear combination of randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest. The methods are implemented into an R package, called PPforest, which is available on CRAN. 

_Keywords_: Random forest, projection pursuit, supervised classification, exploratory data analysis, data mining, visualization


## Introduction

A random forest to utilize combinations of variables in the tree construction, which we call the projection pursuit classification random forest (PPforest) will be describe. For each split a random sample of variables is selected and a linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes. The PPforest uses the PPtree algorithm, which fits a single tree to the data. Utilizing linear combinations of variables to separate classes takes the correlation between variables into account, and can outperform the basic forest when separations between groups occurs on combinations of variables. Two projection pursuit indexes, LDA and PDA, are used for PPforest.

Projection pursuit random forest algorithm description

1. Let N the number of cases in the training set, bootstrap samples from the training set are taking (samples of size N with replacement)

2. For each bootstrap sample a PPtree type algorithm is grown to the largest extent possible. The original PPtree algorithm select in each node the optimal one-dimension projection for separating all classes in the data. PPforest need a modification of PPtree algorithm,  we select for each split a random sample instead of using all the variables and after that in each node we select the optimal one-dimension projection for separating all classes.

3. Let M the number of input variables, a number of $m<<M$ variables are selected at random at each node and each split is based on a linear combination of these randomly chosen variables. The linear combination is computed by optimizing a projection pursuit index, to get a projection of the variables that best separates the classes.

We present two examples using real data to show how PPforest package works.

##Usage
The following table has the PPforest functions with a description.

| Function |Description |
| ------ | ------ | -----: |
|  var_select  |Index id for variables set, sample variables without replacement with constant sample proportion |
|  train_fn |Index id for training set, sample in each class with constant sample proportion. |
|PPtree_split|Projection pursuit classification tree with random variable selection in each split|
|trees_pp| Projection persuit trees for bootstrap samples|
|importance|Data frame with the 1D projection of each split unweighted and weighted by 1-(oob error rate) for each tree and plots of importance variable for each split.|
|PPforest|Run a Projection Pursuit forest|
|forest_ppred|Vector with predicted values from a PPforest|

##Examples
Data sets:

1. NCI 60: Gene expression data set, contains 61 observations and 30 feature variables. Class variable has 8 different tissue types, 9 breast, 5central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non- small-cell lung carcinoma (NSCLC), 6 ovarian and 9 renal. There are 6830 genes.
 
2.Crab: contains 200 observations from two species (blue and orange) and for each specie (50 in each one) there are 50 males and 50 females. Class variable has 4 classes with the combinations of specie and sex (BM, BF, OM and OF). There are 5 continuous feature variables. 
FL is the size of the frontal lobe, RW is rear width, CL is carapace length, CW width and BD body depth.

The PPforest function runs a projection pursuit random forest, the argument testap=TRUE allows to use test data. In this example we split the data in training (2/3) and test (1/3).  
```{r}
library(PPforest)
set.seed(143)


 training.id <- train_fn(NCI60[, 1], 2/3)
 test.id <- as.vector(1:length(NCI60[, 1]))[!(1:length(NCI60[, 1]) %in% (sort(training.id$id)))]
 training <- NCI60[sort(training.id$id), ]
 test <- NCI60[-training.id$id, ]
 pprf.nci60 <- PPforest(train = training, testap = TRUE, test = test, m = 500, size.p = .3, 
              PPmethod = 'PDA', strata = TRUE, lambda=.7)
str(pprf.nci60,max.level=1)
```


